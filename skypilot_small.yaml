name: verl-interactive-multinode

resources:
  accelerators:
    L4: &gpu_count 2
    RTX6000-Ada: *gpu_count
    RTX4090: *gpu_count
    RTX3090: *gpu_count
    RTXA4000: *gpu_count
    A6000: *gpu_count
    T4: *gpu_count
    A10: *gpu_count
    A100: *gpu_count
    A40: *gpu_count
  #image_id: docker:hiyouga/verl:ngc-th2.6.0-cu126-vllm0.8.4-flashinfer0.2.2-cxx11abi0
  image_id: docker:pytorch/pytorch:2.8.0-cuda12.6-cudnn9-devel

num_nodes: 1

workdir: .

envs:
  WANDB_API_KEY: f66373399824f8c1942490b0862a664b8afc8802

file_mounts:
  /workspace/data:
    name: data-0803
    source: /Users/dzorlu/data
    store: gcs
    mode: COPY

  /workspace/checkpoints:
    name: checkpoints-0803
    source: /Users/dzorlu/checkpoints
    store: gcs
    mode: COPY
    persistent: True

  /workspace/logs:
    name: logs-0803
    source: /Users/dzorlu/logs
    store: gcs
    mode: COPY
    persistent: True

  /workspace/code:
    name: code-0803
    source: .
    store: gcs
    mode: COPY
    persistent: True

setup: |
  apt update && apt install -y git tmux htop iperf3 netcat-openbsd

run: |

  # Use default environment, not skypilot-runtime
  export PATH="/usr/local/bin:/usr/bin:$PATH"
  
  export WANDB_API_KEY=f66373399824f8c1942490b0862a664b8afc8802
  export VLLM_USE_V1=1
  echo 'export WANDB_API_KEY=f66373399824f8c1942490b0862a664b8afc8802' >> ~/.bashrc
  echo 'export VLLM_USE_V1=1' >> ~/.bashrc

  cd /workspace/verl
  pip install -e . --no-deps

  HEAD_IP=$(echo "$SKYPILOT_NODE_IPS" | head -n1)
  # Persist the head IP for SSH sessions
  echo "export RAY_HEAD_IP=$HEAD_IP" >> ~/.bashrc
  echo "export RAY_ADDRESS=$HEAD_IP:6379" >> ~/.bashrc

  if [ "$SKYPILOT_NODE_RANK" = "0" ]; then
    ray start --head --disable-usage-stats --port=6379
    sleep 5
  else
    sleep 60
    ray start --address $HEAD_IP:6379 --disable-usage-stats
    sleep 5
  fi

  echo "âœ… Ray cluster up. You can now SSH in and run your VERL job interactively."